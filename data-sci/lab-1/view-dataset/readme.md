# Лабораторна робота №1

Тема: Підсумкові функції та відображення

Мета: Навчитися вибрати відповідні дані
з DataFrame або Series, вибирати правильні дані з набору даних.

## Короткі теоретичні відомості

### Мова Пайтон

#### Історія створення

Python є однією з найпопулярніших мов програмування, відомою своєю простотою і широкими можливостями. Її створив Гвідо ван Россум, голландський програміст, який почав працювати над мовою в кінці 1980-х років. Проект стартував у 1989 році під час різдвяних канікул, коли Гвідо хотів розробити нову мову для написання скриптів, що була б легшою для розуміння та використання, ніж інші мови на той час.

Головна ідея, що лежить в основі створення Python, полягала у наданні програмістам можливості писати читабельний код, з акцентом на простоту та зручність. Назва "Python" походить не від змії, як багато хто думає, а від комедійного шоу BBC "Monty Python's Flying Circus", яке Гвідо дуже любив.

Python випустили у 1991 році, і з того часу мова зазнала значних змін. Протягом багатьох років було випущено кілька версій, і кожна з них додавала нові можливості. Найважливіший момент у розвитку Python стався в 2008 році з випуском Python 3.0, що вніс значні зміни, які зробили його ще зручнішим для програмістів, хоча й несумісним з попередніми версіями.

#### Основні особливості мови Python

Python відрізняється кількома важливими характеристиками, що роблять його унікальним серед інших мов програмування:

1. Інтерпретована мова: Python є інтерпретованою мовою, що означає, що код виконується лінійно, рядок за рядком, без необхідності попередньої компіляції. Це робить розробку швидшою та зручнішою, оскільки програмісти можуть одразу бачити результат виконання своїх програм.

2. Кросплатформеність: Python працює на багатьох платформах, включаючи Windows, macOS, Linux, і навіть мобільні пристрої. Це дозволяє використовувати Python для розробки програм, що працюють на різних системах, без потреби в переписуванні коду.

3. Простий синтаксис: Одна з ключових особливостей Python — це його простий і читабельний синтаксис. Використання відступів замість фігурних дужок для визначення блоків коду спрощує написання і розуміння програм.

4. Динамічна типізація: Python використовує динамічну типізацію, що означає, що тип змінної визначається під час виконання програми, а не під час її написання. Це дозволяє програмістам не турбуватися про декларацію типів для змінних.

5. Автоматичне управління пам'яттю: Python автоматично керує пам'яттю через механізм збору сміття, звільняючи програмістів від необхідності вручну керувати ресурсами пам'яті, як це потрібно в деяких інших мовах, таких як C або C++.

6. Величезна бібліотека модулів: Python має вбудовану стандартну бібліотеку, що містить велику кількість корисних модулів для різних завдань: від роботи з файловими системами та мережами до веб-програмування і наукових розрахунків. Крім того, існує величезна кількість сторонніх бібліотек, доступних через PyPI (Python Package Index).

#### Як працює інтерпретатор Python

Інтерпретатор Python — це програма, яка виконує код, написаний мовою Python. Він приймає текстові файли з кодом (зазвичай з розширенням `.py`), аналізує їх і виконує команди по черзі. Процес роботи інтерпретатора можна розділити на кілька етапів:

1. Лексичний аналіз: Спочатку інтерпретатор читає сирцевий код і перетворює його в послідовність токенів — це окремі елементи коду, такі як ключові слова, ідентифікатори, оператори тощо.

2. Синтаксичний аналіз: На цьому етапі інтерпретатор перевіряє токени на відповідність синтаксичним правилам мови Python, створюючи синтаксичне дерево.

3. Компілікація в байт-код: Python не виконує код безпосередньо, він спочатку компілює його у внутрішнє подання, відоме як байт-код. Байт-код — це більш абстрактна і спрощена форма інструкцій, які можуть бути виконані віртуальною машиною Python (PVM).

4. Виконання байт-коду: Віртуальна машина Python (PVM) виконує байт-код, який був створений на попередньому етапі. Віртуальна машина є частиною інтерпретатора і відповідає за виконання коду на рівні операційної системи.

5. Збирання сміття (Garbage Collection): Під час виконання програми Python постійно слідкує за використанням пам'яті і звільняє її, коли об'єкти більше не потрібні. Це дозволяє програмам працювати ефективніше і запобігає витокам пам'яті.

#### Сценарії використання Python

Python широко використовується в багатьох галузях:

- Веб-розробка: Завдяки фреймворкам, таким як Django і Flask, Python активно використовується для створення веб-додатків.
- Аналіз даних та машинне навчання: Python — вибір номер один серед науковців і аналітиків, оскільки має потужні бібліотеки для роботи з даними, такі як NumPy, Pandas, Matplotlib, а також для машинного навчання (TensorFlow, Scikit-learn).
- Автоматизація завдань: Python часто використовують для автоматизації рутинних завдань, таких як обробка файлів, мережеві операції або навіть тестування програмного забезпечення.

#### Висновок

Python є універсальною мовою програмування з простим синтаксисом, що робить її підходящою для програмістів будь-якого рівня досвіду. Від його створення в 1991 році до сьогодні, Python продовжує розвиватися і використовуватися у багатьох галузях, забезпечуючи зручні і ефективні інструменти для розробників.

### Бібліотека Pandas

#### Вступ

Бібліотека Pandas є однією з найпотужніших бібліотек для роботи з даними в мові Python. Вона була розроблена як зручний інструмент для аналізу та маніпуляції даними, особливо у випадках роботи з таблицями і структурованими даними. Основним об'єктом в Pandas є DataFrame — структура, що дозволяє ефективно зберігати та оперувати табличними даними з різними типами стовпців. Pandas широко використовується в таких сферах, як фінансовий аналіз, машинне навчання, бізнес-аналітика, наукові дослідження та багато інших.

#### Внутрішня реалізація на C

Pandas побудована на основі бібліотеки NumPy, яка написана на C і Fortran. Це означає, що багато операцій, які здаються повільними при роботі з великими наборами даних, насправді дуже швидко виконуються завдяки низькорівневому коду на C. Операції з масивами в Pandas, зокрема математичні операції над стовпцями або рядками, виконуються через виклики до оптимізованих функцій NumPy. Кожен елемент DataFrame зберігається в безперервній області пам'яті завдяки структурі даних NumPy, що дозволяє швидко виконувати доступ до даних та їх обробку.

Ключовою концепцією в Pandas є використання об'єктів блоків пам'яті (block-based memory). Дані зберігаються у вигляді блоків, де кожен блок відповідає за окремий тип даних (цілі числа, числа з плаваючою комою, текст тощо). Це дозволяє досягти високої ефективності при маніпуляціях з великими наборами даних, оскільки можна оптимізувати операції під кожен тип даних окремо.

У бібліотеці Pandas використовується ефективне управління пам'яттю через техніки reference counting і garbage collection Python. Reference counting (підрахунок посилань) забезпечує, що кожен об'єкт має лічильник активних посилань на нього. Коли лічильник досягає нуля, об'єкт звільняється з пам'яті. Проте для циклічних посилань (коли об'єкти ссилаються один на одного) використовується garbage collection, що дозволяє очищати ці об'єкти після того, як вони більше не використовуються.

#### Алокація пам'яті

Python використовує власний менеджер пам'яті, але для великих числових операцій Pandas покладається на NumPy, яка напряму працює з C для алокації пам'яті. Наприклад, для створення великого масиву чисел, Pandas запитує виділення безперервного блоку пам'яті через C-функції, такі як `malloc()`, для оптимізації доступу та зменшення накладних витрат. Окрім цього, всі внутрішні структури Pandas зберігаються у вигляді C-структур для підвищення ефективності.

Якщо говорити про складні операції, такі як злиття або розбиття даних, Pandas спочатку викликає низькорівневі C-алгоритми для сортування та об'єднання даних, що дозволяє значно прискорити виконання.

#### Алгоритми сортування і об'єднання

Pandas використовує різні оптимізовані алгоритми для сортування та об'єднання даних, такі як алгоритми злиття масивів і сортування на основі порівняння. Для об'єднання таблиць за ключами застосовуються спеціалізовані алгоритми на основі хеш-таблиць або дерев пошуку. Це дозволяє ефективно виконувати операції типу join і merge, навіть на великих наборах даних.

Алгоритм об'єднання масивів або таблиць у Pandas працює на основі концепції broadcasting, яка дозволяє застосовувати операції до масивів різної форми без необхідності створювати зайві копії даних. Broadcasting значно зменшує кількість необхідної пам'яті та кількість операцій над даними, що знижує час виконання.

#### Паралельна обробка даних

Хоча Python має обмеження на багатопотоковість через Global Interpreter Lock (GIL), Pandas може використовувати багатопоточність за допомогою бібліотеки Cython або через низькорівневі виклики до C-функцій. Це особливо корисно для обробки великих масивів даних, де кожен етап обробки може бути розподілений на кілька ядер процесора для прискорення виконання.

#### Управління виключеннями та обробка помилок

Pandas реалізує стратегії обробки виключень через типові механізми Python, але при цьому додає можливості гнучкої обробки помилок, специфічних для операцій з даними. Наприклад, функції, які виконують операції над даними, часто дозволяють встановлювати параметри для поведінки у випадку помилок (наприклад, пропускати рядки з помилками або замінювати їх значеннями за замовчуванням). Це забезпечується за допомогою обробки спеціальних винятків у низькорівневому C-коді.

#### Підсумок

Pandas є однією з найпопулярніших бібліотек Python для обробки даних завдяки своїй ефективності та гнучкості. Внутрішня реалізація на базі C та NumPy забезпечує високу швидкість і оптимізацію пам'яті. Ефективні алгоритми сортування та об'єднання даних, а також низькорівневі оптимізації дозволяють обробляти великі набори даних з високою продуктивністю.

### Важливі для роботи функції

#### Read CSV

Функція `pd.read_csv()` — це один з найпоширеніших способів завантаження даних у форматі CSV (Comma Separated Values) у DataFrame в бібліотеці Pandas. CSV — це формат файлів, що зберігає табличні дані у вигляді тексту, де кожен рядок представляє один запис, а значення розділяються комами або іншими роздільниками (наприклад, табуляціями чи крапками з комами).

##### Що таке CSV

Файл CSV широко використовується для обміну даними між системами через його простоту та легкість для обробки. Кожен рядок у файлі CSV відповідає одному запису, а значення у рядку відповідають окремим полям, розділеним комами (або іншим роздільником). Цей формат дозволяє легко передавати та зберігати дані в табличній формі, яку можна імпортувати в різні програми, зокрема в системи аналізу даних.

##### Як працює `pd.read_csv()`

Коли викликається `pd.read_csv()`, Python відкриває файл і починає його читання рядок за рядком. Кожен рядок парситься і розбивається на окремі значення відповідно до роздільника (як правило, кома). Ці значення використовуються для заповнення DataFrame, який є двовимірною структурою даних, подібною до таблиці, де рядки представляють записи, а стовпці — поля.

Після розбиття рядків на окремі значення, Pandas створює DataFrame, що складається з двох основних частин: індексу та даних. Індекс — це набір чисел або ідентифікаторів, які представляють кожен рядок у таблиці. Дані — це структура, де кожен стовпець містить конкретне поле з набору даних.

##### Структура DataFrame у пам'яті

Під час створення DataFrame в пам'яті, Pandas використовує багатокомпонентну архітектуру на основі мови C. DataFrame базується на масивах NumPy, які є високопродуктивними багатовимірними масивами. NumPy масиви, у свою чергу, використовують низькорівневі механізми для виділення пам'яті та роботи з нею на рівні мови C. Коли DataFrame створюється з CSV файлу, кожен рядок і стовпець у масиві зберігається у вигляді послідовних блоків пам'яті, що дозволяє швидко здійснювати доступ до даних і обробляти їх.

##### Як виглядає процес читання CSV у пам'яті

1. Алокація пам'яті: коли файл CSV зчитується, інтерпретатор Python (через Pandas) виділяє пам'ять для зберігання даних у вигляді масивів. Кожен стовпець отримує окремий масив, де зберігаються всі його значення. Це робиться за допомогою механізмів NumPy, які оперують пам'яттю на низькому рівні, використовуючи C-функції для оптимізації процесу.
  
2. Типи даних: Pandas аналізує кожен стовпець і визначає оптимальний тип даних для нього. Наприклад, числові стовпці зберігаються як масиви типу `int32` або `float64`, а текстові — як об'єкти (стрічки). Це допомагає зекономити пам'ять і забезпечити ефективну обробку даних.

3. Алгоритми парсингу: сам процес розбору CSV файлу здійснюється з використанням оптимізованих алгоритмів, написаних на C. Вони забезпечують швидке розбиття рядків і перетворення їх у масиви, що зберігаються у DataFrame.

4. Робота з великими обсягами даних: Pandas може використовувати індексацію та вибірку, щоб ефективно обробляти великі набори даних. Завдяки багаторівневій структурі пам'яті і підтримці кешування, операції на великих DataFrame виконуються швидко, навіть якщо файл CSV містить мільйони рядків.

##### Висновок

Функція `pd.read_csv()` є ключовим інструментом Pandas для завантаження даних у форматі CSV. Завдяки використанню NumPy та низькорівневих оптимізацій на C, Pandas забезпечує швидку та ефективну обробку великих наборів даних, створюючи DataFrame як багатовимірні масиви з числовими або текстовими значеннями.

#### DataFrame.head()

Функція `DataFrame.head()` в бібліотеці Pandas використовується для того, щоб вивести перші кілька рядків DataFrame. Ця функція за замовчуванням повертає перші п’ять рядків, однак користувач може передати будь-яке число рядків як аргумент. Це корисно для швидкого перегляду структури та вмісту даних, особливо під час дослідження великих наборів даних.

##### Опис функції

Коли користувач викликає функцію `head()`, Pandas відбирає перші кілька рядків з основної структури DataFrame, яка по суті є складною оболонкою над масивами NumPy. Кожен стовпець у DataFrame — це масив, створений з NumPy, а кожен рядок виводиться як окрема порція даних. Оскільки DataFrame є індексованим об'єктом, це дозволяє легко отримати доступ до конкретних рядків або стовпців.

##### Як працює індексація в DataFrame

Під капотом, індексація в DataFrame базується на принципах, реалізованих у масивах NumPy. Масиви NumPy є одновимірними або багатовимірними структурами, які працюють з великими об'ємами даних на основі низькорівневих механізмів, реалізованих на C. Коли викликається функція `head()`, вона не просто копіює дані, а повертає частину існуючого масиву з індексацією.

- Індексація в NumPy працює на основі системи координат. Кожен елемент масиву має індекс, який відповідає його положенню у масиві. Індекси в NumPy масивах починаються з нуля. При виклику `head()` відбувається звернення до перших n елементів, що швидко виконується завдяки використанню векторних операцій, оптимізованих для C.
  
- Індекси в DataFrame можуть бути числовими або змішаними (наприклад, мати форму індексу на основі тексту), але при виконанні операцій, таких як `head()`, відбувається безпосередній доступ до першого сегменту даних в базовій структурі NumPy.

##### Реалізація на C

NumPy є основою для реалізації масивів Pandas. Цей модуль, написаний на C, оптимізований для швидкої обробки даних. Масиви в NumPy використовують безперервну алокацію пам'яті, яка дозволяє ефективно зберігати та оперувати даними. Коли викликається функція `head()`, вона звертається до цієї базової структури, використовуючи ефективний доступ до пам’яті через NumPy.

Масиви NumPy в пам’яті — це блочна структура, де кожен блок відповідає певному стовпцю DataFrame. Алокація пам'яті для таких масивів здійснюється з мінімальним накладними витратами завдяки використанню низькорівневих механізмів на C. Таким чином, функції, як `head()`, можуть швидко вибирати частини даних, не створюючи додаткових копій.

##### Алгоритми і реалізація

Коли ви викликаєте функцію `head()`, відбувається ітерація по індексам рядків, що зберігаються в базовій структурі масивів. Алгоритми, що використовуються для цієї операції, оптимізовані для швидкого доступу до даних і їх вибірки без надмірної алокації пам'яті. Завдяки інтеграції з C, Pandas здатен виконувати ці операції майже на рівні низькорівневих мов програмування, таких як C або C++.

##### Висновок

Функція `DataFrame.head()` є важливим інструментом для перегляду даних у Pandas, яка використовує можливості NumPy для швидкого доступу до даних і їх відображення. Завдяки використанню низькорівневої оптимізації на C, Pandas може ефективно працювати з великими масивами даних, дозволяючи користувачам швидко отримувати інформацію про структуру та вміст DataFrame.

#### DataFrame.describe()

Функція `DataFrame.describe()` в бібліотеці Pandas використовується для обчислення основних статистичних показників по числових стовпцях DataFrame. Вона дає змогу швидко отримати уявлення про дані, включаючи такі показники, як середнє значення, стандартне відхилення, мінімум, максимум та квартилі. Ця функція є дуже важливою при попередньому аналізі даних, оскільки вона дозволяє побачити загальні статистичні тенденції і варіації в наборі даних.

##### Опис функції

Коли ви викликаєте `DataFrame.describe()`, вона автоматично вибирає всі числові стовпці в DataFrame і виконує для них кілька обчислень:

- Count: Кількість значень у кожному стовпці. Це просто кількість рядків, де є реальні дані, без урахування відсутніх або null-значень.
  
- Mean: Середнє значення для кожного стовпця. Це стандартна арифметична сума всіх елементів стовпця, поділена на кількість елементів.

- Standard Deviation (std): Стандартне відхилення показує, наскільки розкидані дані в кожному стовпці. Це квадратний корінь з дисперсії, що є мірою розкиданості чисел від середнього значення.

- Min і Max: Мінімальне та максимальне значення для кожного стовпця.

- 25%, 50%, 75%: Значення, що відповідають першому, другому (медіані) і третьому квартилям. Це своєрідні "поділи" набору даних на чотири рівні частини.

##### Математичні концепції

1. Середнє арифметичне (Mean):
   Формально, середнє арифметичне для набору даних можна виразити як:

   ${Mean} = \frac{1}{n} \sum_{i=1}^{n} x_i$

   Де $(n)$ — кількість елементів у стовпці, а $(x_i)$ — кожне окреме значення в наборі даних. Середнє арифметичне є важливим показником центральної тенденції, що допомагає зрозуміти, де знаходиться середнє значення набору даних.

2. Стандартне відхилення (Standard Deviation):
   Стандартне відхилення обчислюється як:

   $\text{std} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_i - \text{Mean})^2}$

   Воно показує, наскільки сильно значення в наборі даних розкидані навколо середнього. Стандартне відхилення є важливим при аналізі варіації даних: низьке значення стандартного відхилення означає, що дані зосереджені навколо середнього значення, а високе — що дані більш розкидані.

3. Медіана (50%):
   Медіана — це середнє значення в упорядкованому наборі даних. Для набору даних з непарною кількістю елементів медіана — це середнє значення, для парної кількості елементів — середнє арифметичне двох центральних значень. Медіана особливо важлива, коли дані містять аномальні або екстремальні значення, оскільки вона не є чутливою до цих екстремальних значень, на відміну від середнього арифметичного.

4. Квартилі (25%, 75%):
   Квартилі — це показники, що ділять набір даних на чотири рівні частини. Перший квартиль (25%) означає, що 25% даних знаходяться нижче цього значення, а третій квартиль (75%) — що 75% даних знаходяться нижче цього значення. Інтерквартильний діапазон (різниця між третім і першим квартилем) є важливим показником для оцінки варіації даних.

##### Реалізація на C і механізми пам'яті

Коли ви викликаєте `DataFrame.describe()`, Pandas під капотом використовує бібліотеку NumPy для обчислення всіх цих статистичних показників. Всі ці операції виконуються через високоефективні векторизовані операції NumPy, написані на C. Це дозволяє обробляти великі масиви даних з мінімальними витратами часу і пам'яті.

- Пам'ять: Алокація пам'яті відбувається через низькорівневі механізми, і вся інформація зберігається в безперервних блоках пам'яті для швидкого доступу.
- Алгоритми: Обчислення середнього, стандартного відхилення та інших показників використовують добре оптимізовані цикли на C, що дозволяє швидко виконувати всі необхідні обчислення навіть для великих наборів даних.

##### Висновок

Функція `DataFrame.describe()` надає корисний огляд набору даних за допомогою кількох статистичних показників, що допомагають зрозуміти розподіл і структуру даних. В основі її роботи лежать механізми NumPy і C, що забезпечують ефективність і високу продуктивність при роботі з великими даними.

#### Series.unique()

Функція `Series.unique()` використовується в Pandas для отримання масиву унікальних значень з об'єкта Series. Це означає, що якщо в серії є повторювані значення, вони будуть відфільтровані, і залишаться лише унікальні елементи. Наприклад, якщо в колонці датафрейму зберігаються назви країн, а деякі з них повторюються, виклик `Series.unique()` поверне список лише унікальних країн.

##### Алгоритм і його робота

1. Концепція роботи:
   Коли викликається функція `unique()`, вона створює новий масив, який буде містити лише унікальні значення з вхідного масиву. Спочатку кожен елемент перевіряється на те, чи вже він присутній у списку унікальних значень. Якщо він не присутній, то додається до цього списку.

2. Основний алгоритм:
   Алгоритм `unique()` за своєю природою працює на основі структури даних, схожої на хеш-таблицю для прискорення пошуку. Основні кроки алгоритму:

   - Ініціалізується порожня структура для зберігання унікальних значень.
   - Для кожного елемента в серії:
     - Перевіряється, чи є елемент у хеш-таблиці (використовуючи вбудовані методи пошуку).
     - Якщо елемент ще не додано, він додається до списку унікальних значень.
   - Після проходження через всі елементи, повертається масив унікальних значень.

3. Реалізація на C (NumPy):
   У Pandas, під капотом використовується бібліотека NumPy, яка є основою для обробки даних. Виконання операцій на рівні NumPy реалізовано на C для оптимізації швидкості та мінімізації використання пам'яті.

   - NumPy використовує під капотом векторизовані операції для швидкої обробки великих масивів даних.
   - У випадку з `Series.unique()`, використовується функція `numpy.unique()`, яка оптимізована для роботи з великими обсягами даних через спеціальні індексаційні структури (наприклад, хеш-таблиці або збалансовані дерева).

4. Алокація пам'яті:
   Пам'ять для масиву унікальних значень виділяється динамічно. Алокація проводиться через низькорівневі операції C, що дозволяє ефективно використовувати пам'ять і забезпечує високу швидкість. NumPy алокує пам'ять для кожного нового значення лише тоді, коли його ще не було зустрінуто в попередніх елементах.

5. Оптимізація та складність алгоритму:
   - Основна операція пошуку у хеш-таблиці займає \(O(1)\) часу, тому для кожного елемента серії в середньому потрібен постійний час на перевірку та додавання.
   - Складність всього алгоритму для отримання унікальних значень — \(O(n)\), де \(n\) — кількість елементів у серії. Це дозволяє функції працювати швидко навіть на великих наборах даних.

##### Висновок

Функція `Series.unique()` є важливим інструментом для роботи з серіями даних у Pandas, що дозволяє швидко отримувати унікальні значення. Завдяки ефективній реалізації на рівні C з використанням векторизованих операцій і хеш-таблиць, ця функція може обробляти великі масиви даних з мінімальними витратами часу й пам'яті.

#### Series.mean()

Функція `Series.mean()` у Pandas використовується для обчислення середнього значення по значеннях об'єкта Series. Це базова статистична функція, яка дозволяє швидко отримати середню арифметичну величину для числових даних. Давайте розглянемо, як працює ця функція, її математичні основи та реалізацію на рівні C.

##### Основи середнього арифметичного

Середнє арифметичне — це сума всіх елементів, поділена на кількість елементів:

$\text{Mean} = \frac{1}{n} \sum_{i=1}^{n} x_i$

Де $(n)$ — кількість елементів, а $(x_i)$ — кожен окремий елемент серії.

У контексті серії Pandas:

1. Сумування — функція сумує всі значення у стовпці, ігноруючи будь-які відсутні або недійсні значення (наприклад, `NaN`).
2. Підрахунок елементів — Pandas автоматично рахує кількість валідних елементів (без `NaN`).
3. Розрахунок середнього — отримується шляхом ділення суми на кількість валідних елементів.

##### Алгоритм і реалізація

1. Концепція роботи:
   Функція `mean()` спочатку використовує метод обчислення суми всіх валідних елементів у серії, потім ділить отримане значення на кількість цих елементів.

2. Обробка відсутніх значень (NaN):
   Pandas ігнорує відсутні значення (`NaN`) під час розрахунку середнього, що дозволяє коректно обробляти серії, де частина даних відсутня.

3. Реалізація на C (NumPy):
   Під капотом Pandas використовує бібліотеку NumPy, яка реалізована на C для високої продуктивності. Функція `mean()` у Pandas викликає відповідну функцію NumPy для обробки масивів, забезпечуючи швидке виконання операції навіть на великих масивах даних.

   - Алокація пам'яті: NumPy працює з масивами на рівні C, що дозволяє ефективно використовувати пам'ять і уникати зайвих копій масивів. Це забезпечує швидке виконання обчислень.
   - Операції з плаваючими точками: NumPy використовує оптимізовані алгоритми для операцій з числами з плаваючою точкою, що зменшує ризик втрати точності при роботі з великими числами.

4. Алгоритм і складність:
   Складність операції `mean()` — $( O(n) )$, де $( n )$ — кількість елементів у серії. Це означає, що час виконання функції пропорційний кількості елементів.

##### Висновок

Функція `Series.mean()` є важливим інструментом для базових статистичних обчислень у Pandas. Вона дозволяє ефективно обчислювати середні значення великих наборів даних завдяки використанню оптимізованих алгоритмів та реалізації на рівні C через NumPy.

#### DataFrame.drop()

Функція `DataFrame.drop()` у Pandas використовується для видалення стовпців або рядків з датафрейму. Вона дозволяє гнучко оперувати структурою даних, даючи можливість зберігати лише необхідну інформацію.

##### Як працює `drop()`?

Коли викликається функція `drop()`, вона створює нову копію DataFrame без вказаних рядків або стовпців. За замовчуванням функція не змінює оригінальний DataFrame, але цей параметр можна змінити, встановивши `inplace=True`. Основні аспекти роботи цієї функції:

1. Операція видалення стовпців або рядків:
   - Видалення рядків здійснюється за індексами, а видалення стовпців — за назвами.
   - Параметр `axis` вказує, що саме видаляти: стовпці (axis=1) або рядки (axis=0).

2. Складність алгоритму:
   Операція має лінійну складність — \(O(n)\), де \(n\) — кількість рядків або стовпців у DataFrame. Це пояснюється тим, що для кожного елемента потрібно пройти по відповідній осі та видалити дані з неї.

3. Алгоритм видалення:
   Функція створює новий DataFrame або Series без вказаних елементів, оскільки всі рядки та стовпці представлені як окремі блоки в пам'яті. Це зумовлює необхідність створення нової структури з виключеними даними.

4. Робота з пам'яттю:
   Функція `drop()` копіює існуючі дані без вказаних елементів у нову область пам'яті. Якщо ж використовується `inplace=True`, то зміни виконуються безпосередньо на існуючому DataFrame, що дозволяє уникнути дублювання пам'яті, але може призвести до втрати даних.

5. Реалізація на рівні C через NumPy:
   Pandas побудований на основі NumPy, який забезпечує роботу з багатовимірними масивами на низькому рівні. Масиви в NumPy, представлені як безперервні блоки пам'яті, дозволяють ефективно маніпулювати даними. При виклику `drop()`, NumPy маніпулює з індексами, забезпечуючи швидкий доступ до рядків або стовпців та їх видалення. Видалені елементи фактично не видаляються негайно з пам'яті, а лише виключаються з доступу за допомогою корекції індексації.

##### Оптимізація та виклики

1. Копіювання даних:
   Оскільки `drop()` зазвичай створює новий DataFrame, це призводить до додаткового використання пам'яті та часу на копіювання. Важливо враховувати, що для великих наборів даних це може бути ресурсомістким процесом.

2. Обробка даних:
   Видалення стовпців або рядків із великої кількості даних — це також операція, яка залучає багато кешування даних і операцій з пам'яттю, що реалізовано через алгоритми низького рівня в C.

3. Алокація та вивільнення пам'яті:
   Після видалення даних, виділені раніше області пам'яті можуть не одразу звільнятися, що залежить від специфіки роботи збирача сміття в Python, а також того, як саме Pandas працює з внутрішніми масивами через NumPy.

Функція `drop()` забезпечує гнучкість у роботі з таблицями, але має свої виклики в контексті управління пам'яттю та ефективності для великих даних.

#### DataFrame.sample()

Метод `DataFrame.sample()` в Pandas використовується для вибірки випадкових рядків або стовпців із DataFrame. Це корисний інструмент для аналізу великих наборів даних, коли потрібно переглянути випадковий піднабір даних або підготувати тестову вибірку для аналізу.

##### Опис роботи `DataFrame.sample()`

Ця функція дозволяє користувачу вибрати випадкову кількість рядків або стовпців із початкового DataFrame. Основні параметри методу включають:

- n: кількість рядків або стовпців для вибірки.
- frac: пропорція від загальної кількості рядків або стовпців, яку потрібно вибрати (значення від 0 до 1).
- replace: якщо встановлено значення `True`, вибірка буде виконуватись із заміщенням, тобто одні й ті самі рядки можуть вибиратись кілька разів.
- axis: вказує, чи вибирати рядки (axis=0) або стовпці (axis=1).

##### Алгоритм і складність

Метод `sample()` використовує алгоритм випадкової вибірки, де для кожного рядка або стовпця генерується випадковий індекс, і на основі цього індексу здійснюється вибірка. Оскільки вибірка проводиться за допомогою вбудованих функцій Python та NumPy, це робить процес дуже швидким навіть для великих обсягів даних.

- Складність операції для вибірки рядків — $(O(n))$, де $(n )$ — кількість рядків у DataFrame.
- Якщо вибірка здійснюється з параметром `replace=True`, кожен рядок може бути вибраний кілька разів, що не збільшує загальну складність операції, але змінює розподіл вибірки.

##### Внутрішня робота в пам'яті та реалізація на C

Pandas побудований на основі NumPy, що забезпечує низькорівневу оптимізацію роботи з даними на рівні C. Для вибірки рядків `sample()` працює через масиви NumPy, зберігаючи дані у вигляді континууму пам'яті, що дозволяє ефективно отримувати доступ до елементів за індексами. Коли викликається метод `sample()`, кожен індекс отримує випадкове значення, після чого вибірка здійснюється через швидкий доступ до даних масиву.

Оскільки дані у DataFrame зберігаються у вигляді 2D масивів, вибірка рядків передбачає роботу з індексацією цих масивів і маніпуляцію індексами для вибору певних рядків.

- У випадку великого об'єму даних, алгоритм використовує оптимізації для мінімізації операцій копіювання, щоб уникнути зайвого навантаження на пам'ять.
- У разі роботи з параметром `replace=False`, NumPy створює масив унікальних випадкових індексів, що зменшує дублювання і навантаження на систему.

##### Висновок

Метод `DataFrame.sample()` дозволяє швидко і ефективно отримувати випадкові вибірки з великого набору даних. Завдяки роботі через NumPy, він реалізований з високою оптимізацією пам'яті та швидкодії, що робить його корисним інструментом для обробки великих датафреймів.

#### DataFrame.sort_values()

`DataFrame.sort_values()` — це метод Pandas, який використовується для сортування даних у DataFrame за одним або кількома стовпцями. Він дозволяє користувачеві сортувати значення у зростаючому або спадаючому порядку на основі конкретного стовпця або індексів.

##### Алгоритм сортування

У Pandas метод `sort_values()` реалізований з використанням алгоритму швидкого сортування (QuickSort) за замовчуванням. Він також підтримує інші алгоритми, зокрема:

- quicksort: Це стандартний алгоритм для сортування, який має середню складність $(O(n \log n))$. Він використовує стратегію розділяй і володарюй, вибираючи опорний елемент і розділяючи список на дві частини — елементи менші та більші за опорний. Після цього викликається рекурсивне сортування обох частин.
  
- mergesort: Це стабільний алгоритм сортування, що об'єднує відсортовані підсписки. Він має найгіршу складність $(O(n \log n))$, що робить його ефективним для великих наборів даних, але він вимагає додаткової пам'яті для злиття.
  
- heapsort: Це ще один стабільний алгоритм сортування, який має складність $(O(n \log n))$ і не вимагає додаткової пам'яті. Він будує максимальну купу (heap) і поступово видаляє максимальні елементи.

##### Реалізація на рівні C і пам'ять

Коли дані сортуються, вони представлені у вигляді масивів NumPy, що забезпечує низькорівневу роботу з пам'яттю. NumPy використовує власні алгоритми для роботи з сортуванням на основі оптимізованих функцій C. Це дозволяє сортувати великі масиви даних у пам'яті ефективно.

Операції сортування можуть викликати виділення нової пам'яті для збереження відсортованих значень, особливо у випадках, коли виконується стабільне сортування (наприклад, mergesort). У разі нестабільного сортування пам'ять може використовуватись більш економно.

##### Використання індексації в процесі сортування

При сортуванні метод `sort_values()` використовує індекси для відстеження позицій рядків у DataFrame. Ці індекси зберігаються в пам'яті як окремі структури даних і маніпулюються відповідно до обраного алгоритму сортування. Для стабільних алгоритмів індекси зберігаються без змін у разі рівних значень, тоді як для нестабільних алгоритмів можуть відбуватися зміни.

##### Висновок

`DataFrame.sort_values()` забезпечує швидке та ефективне сортування даних із використанням високоефективних алгоритмів, таких як QuickSort, MergeSort і HeapSort. Реалізація на рівні C та використання NumPy дозволяють оптимізувати роботу з пам'яттю і забезпечити максимальну продуктивність при обробці великих наборів даних.

#### DataFrame.apply()

Метод `DataFrame.apply()` у Pandas дозволяє користувачеві застосувати функцію до кожного стовпця або рядка DataFrame. Це надзвичайно гнучкий інструмент для обчислень, трансформацій та агрегування даних у DataFrame. Метод є потужним завдяки тому, що користувач може передати будь-яку функцію для застосування до елементів даних.

##### Алгоритм роботи

`apply()` виконує ітерацію над рядками або стовпцями DataFrame в залежності від значення аргументу `axis`. Якщо `axis=0`, функція застосовується до кожного стовпця, якщо `axis=1`, функція застосовується до кожного рядка. Це дозволяє виконувати широкий спектр операцій, починаючи від обчислення на рівні рядків і закінчуючи складними агрегатами на рівні стовпців.

##### Важливі моменти роботи

- Обчислення на рівні стовпців/рядків: Кожен стовпець або рядок DataFrame передається до функції, яку вказав користувач. Це може бути стандартна функція Python, або ж lambda-функція для швидких трансформацій. При цьому функція може виконувати будь-які операції над рядком або стовпцем.
  
- Оптимізація з використанням NumPy: Метод `apply()` активно використовує NumPy для швидкої роботи з даними на низькому рівні. Під час обробки великих масивів даних Pandas використовує векторизацію та швидку обробку на основі C, що дозволяє швидко виконувати операції на рівні пам'яті.

##### Реалізація на рівні C

На низькому рівні, як і більшість операцій у Pandas, `apply()` використовує потужність бібліотеки NumPy для роботи з багатовимірними масивами. NumPy, у свою чергу, написаний на C для забезпечення високої продуктивності при маніпуляціях із великими масивами даних. Під час роботи метод `apply()` виконує цикли на рівні C для ефективної обробки даних.

У пам'яті дані DataFrame представлені як багатовимірні масиви, кожен з яких містить відповідні стовпці або рядки. Кожна ітерація над рядками або стовпцями викликає функцію, яка обчислюється для кожного елемента або їх групи.

##### Використання

Метод дозволяє виконувати такі операції:

- Перетворення та обробка значень у стовпцях
- Застосування складних математичних функцій до кожного елемента або рядка
- Побудова нових стовпців на основі обчислень із наявних даних

##### Висновок

Метод `DataFrame.apply()` забезпечує високу гнучкість та ефективність для користувачів Pandas, дозволяючи застосовувати кастомні функції до даних у DataFrame. Завдяки низькорівневій реалізації на базі C через NumPy, цей метод підтримує швидкі та ефективні обчислення навіть для великих наборів даних.

#### Series.value_counts()

Метод `Series.value_counts()` в Pandas використовується для підрахунку унікальних значень у серії (стовпці), тобто він показує, скільки разів кожне значення зустрічається у даній серії. Це дозволяє легко отримати частотний розподіл для категорійних або будь-яких інших дискретних даних.

##### Алгоритм роботи

Коли викликається метод `value_counts()`, Pandas виконує ітерацію по кожному елементу серії та підраховує кількість його появ. Основний процес можна описати таким чином:

1. Ініціалізація підрахунків: Створюється порожній словник або подібна структура для зберігання унікальних значень та їх кількості.
2. Ітерація по елементах серії: Для кожного елемента серії перевіряється, чи був він вже зареєстрований у словнику. Якщо значення вже є в словнику, його лічильник збільшується на одиницю. Якщо ні, значення додається у словник з початковим лічильником, рівним 1.
3. Сортування результатів: Після завершення підрахунку метод сортує результати за кількістю появ за спаданням (значення з найбільшою частотою показуються першими). Це за замовчуванням, але користувач може змінити порядок сортування.

##### Важливі параметри

- normalize: Якщо цей параметр встановлено як `True`, то метод повертає частоти у вигляді відсотків замість кількості появ.
- sort: Визначає, чи слід сортувати значення за кількістю появ.
- ascending: Якщо `True`, значення будуть відсортовані за зростанням.
- dropna: Якщо `False`, метод буде враховувати пропущені значення (NaN).

##### Реалізація на рівні C та Python

На низькому рівні цей метод використовує структури даних із Python та C для забезпечення ефективності. Оскільки Pandas побудований на базі NumPy, то більшість операцій оптимізовано завдяки векторизації та використанню багатовимірних масивів на рівні C.

- Пам'ять: У пам'яті дані представлені як масиви (зокрема, масиви NumPy), а підрахунок частот виконується за допомогою швидких операцій на рівні масивів. Використання таких структур знижує кількість витрат на створення нових об'єктів і дозволяє швидко працювати з великими наборами даних.
- Оптимізація: Pandas оптимізує цей процес шляхом мінімізації кількості операцій, що виконуються на Python-рівні. Використання словників та інших структур даних на рівні C робить цей метод дуже швидким і масштабованим.

##### Висновок

Метод `value_counts()` є дуже потужним інструментом для роботи з категорійними даними та підрахунку частот унікальних значень. Завдяки своїй оптимізації на основі C через NumPy, цей метод здатен обробляти великі набори даних швидко та ефективно.

#### DataFrame.iloc[]

Метод `DataFrame.iloc[]` в Pandas використовується для індексації та доступу до рядків і стовпців у DataFrame за їх числовими індексами. Цей метод підтримує індексацію з використанням як окремих індексів, так і діапазонів індексів (подібно до індексації масивів у Python), що дозволяє гнучко отримувати доступ до даних у таблиці.

##### Алгоритм роботи

1. Ініціалізація: Використовуючи метод `iloc[]`, користувач задає індекси рядків і стовпців, до яких він хоче отримати доступ. Індексація виконується аналогічно індексації списків у Python, починаючи з індексу 0 для першого елемента.

2. Пошук за індексами: Після отримання індексів Pandas звертається до відповідних блоків пам'яті, де зберігаються дані DataFrame. Як правило, у пам'яті ці дані представляються у вигляді NumPy масивів, де кожен стовпець зберігається як окремий масив. Це дозволяє легко та швидко доступатись до потрібних рядків і стовпців.

3. Підтримка діапазонів: Користувач може звертатися до діапазону індексів через синтаксис, подібний до `start:stop:step`, що дозволяє, наприклад, витягнути певний підмножину даних або рядків із таблиці. Також підтримується від'ємна індексація, яка дозволяє починати підрахунок індексів з кінця масиву (наприклад, індекс -1 вказує на останній рядок).

##### Робота з пам'яттю на рівні C

На рівні реалізації в C, метод `iloc[]` використовує низькорівневі механізми для доступу до даних у пам'яті. Кожен стовпець DataFrame за своєю суттю є окремим блоком пам'яті, який оптимізовано для швидкого доступу до елементів. Вся структура побудована на базі бібліотеки NumPy, яка використовує ефективні структури даних у пам'яті для представлення масивів.

- Пам'ять: У DataFrame кожен стовпець зберігається як NumPy масив, що забезпечує швидкий доступ до кожного елемента. Коли використовується `iloc[]`, Pandas працює безпосередньо з цими масивами, обходячи механізми пошуку за мітками (labels), що характерно для іншого методу `loc[]`. Це прискорює процес індексації.
  
- Алокація: Якщо повертаються нові підмножини даних (наприклад, кілька рядків), вони зберігаються у новій структурі DataFrame або Series, для якої виділяється новий блок пам'яті. Це означає, що виклик методу `iloc[]` не змінює оригінальну таблицю, а створює новий об'єкт з копією або видом (view) на дані.

##### Висновок

Метод `iloc[]` є одним із ключових інструментів для роботи з індексацією в DataFrame, дозволяючи швидко і ефективно отримувати доступ до необхідних рядків і стовпців за допомогою числових індексів. Завдяки використанню NumPy і векторизованих операцій, цей метод оптимізований для роботи з великими масивами даних.

## Завдання до лабораторної роботи №1

1. Детально розглянути наведені теоретичні відомості до даної лабораторної роботи.
   - Мета полягає у вивченні теоретичних аспектів, які дозволять глибше зрозуміти роботу з наборами даних у Python та використання бібліотек Pandas і NumPy.

2. Ознайомитися з кодом, що наведений у Jupyter notebook `DD_lab1.ipynb`.
   - Виконати огляд наданого коду в `Jupyter notebook`, звертаючи увагу на використані функції для обробки та аналізу даних.

3. Виберіть набір даних відповідно до номеру у списку з таблиці 1.
   - Для кожного варіанту студента в таблиці 1 вказано конкретний набір даних, що підлягає обробці.

4. Застосувати до вибраного набору всі підсумкові функції та методи відображень, які розглянуті у теоретичних відомостях до лабораторної роботи.
   - Перевірити роботу таких функцій як `.describe()`, `.mean()`, `.unique()` тощо для аналізу даних.

5. Результат виконання лабораторної роботи має містити:
   - A) Звіт про виконання лабораторної роботи. Звіт обов’язково має містити обґрунтоване пояснення дій щодо конкретних результатів роботи підсумкових функцій та відображень.
   - Б) Jupyter notebook файл з кодом та результатами обробки даних.

## Хід роботи

### Отримання датасету

![Рисунок 3.1 - Отримання датасету](image.png)

На рисунку вище зображено процес завантаження даних із CSV-файлу за допомогою функції `pd.read_csv()`. Ми передаємо шлях до файлу з датасетом через змінну `file_path`, а функція з бібліотеки Pandas зчитує цей файл та перетворює його на об'єкт типу `DataFrame`. Це дозволяє легко працювати з даними у вигляді таблиці.

#### Обґрунтування використання функції `read_csv()`

Функція `read_csv()` є стандартним методом для завантаження даних із файлів CSV у Python. CSV-файли є одним із найбільш популярних форматів для зберігання табличних даних через свою простоту та легкість у використанні. Ця функція створює об'єкт `DataFrame`, який дозволяє зручно працювати з рядками та стовпцями даних, виконувати фільтрацію, сортування та інші маніпуляції. Додатковий теоретичний матеріал щодо використання функції заходиться в коротких теоретичних відомостях [[Read CSV]](#read-csv)

### Аналіз датасету

Для початку аналізу даних у даному лабораторному завданні ми можемо скористатися функціями Pandas для отримання статистичних характеристик датасету. Однією з ключових функцій, яку ми використовуємо на даному етапі, є `DataFrame.describe()`. Ця функція дозволяє отримати статистичний огляд кожного числового стовпця у датафреймі, як це можна побачити на рисунку вище.

Функція `describe()` надає інформацію про кількість значень у стовпці (count), середнє значення (mean), стандартне відхилення (std), мінімальне та максимальне значення (min, max), а також квартилі (25%, 50%, 75%). У цьому випадку, ми отримали статистичну інформацію для стовпців `Rank` і `Cancer_Rate`, що дозволяє нам зробити базові висновки про розподіл значень у датасеті.

Функція `describe()` є найбільш підходящою для початкового аналізу, оскільки вона дозволяє отримати базову інформацію про розподіл значень у колонках, що дає можливість зрозуміти структуру даних та знайти аномалії чи відхилення. З теоретичним матеріалом щодо цієї функції можна ознайомитися в розділі [[DataFrame.describe()]](#dataframedescribe).

![Рисунок 3.2 - Аналіз результатів](image-1.png)

На основі отриманих результатів ми можемо зробити наступні висновки:

- Кількість значень у кожному стовпці однакова (50), що говорить про відсутність пропущених значень.
- Середнє значення рівня захворюваності на рак дорівнює 294.08, що вказує на загальний рівень захворюваності для вибірки.
- Стандартне відхилення показує, наскільки сильно варіюються значення від середнього (50.02), що свідчить про відносно малу варіацію рівня захворюваності.
- Мінімальне та максимальне значення (233.6 і 468.0 відповідно) вказують на діапазон рівнів захворюваності серед вибраних країн.

### Унікальні країни

![Рисунок 3.3 - Перегляд унікальних країн](image-2.png)

На рисунку представлено результат виконання функції `Series.unique()`, яка використовується для отримання унікальних значень у стовпці "Country" набору даних. Дана функція повертає масив унікальних значень, зберігаючи їх порядок появи.

Функція `unique()` є корисною для виявлення всіх унікальних елементів у даних, наприклад, у нашому випадку для виявлення переліку країн у наборі даних про рівень захворюваності на рак. Це дозволяє отримати загальний огляд даних та визначити кількість різних країн, представлених у цьому наборі даних.

Функція `unique()` обирається тому, що вона дозволяє швидко та ефективно отримати перелік всіх унікальних значень у зазначеному стовпці, без необхідності додаткового сортування або складних операцій. Окрім того, її використання є важливим на етапі попередньої обробки даних для розуміння їх структури.

Теоретичне обґрунтування функції `unique()` наведено у відповідному розділі [[Series.unique()]](#seriesunique)

### Знаходження середнього рівня захворюваності

![Рисунок 3.4 - Середній рівень захворюваності](image-3.png)

На рисунку показано, як використовується метод `mean()` для обчислення середнього значення рівня захворюваності на рак. Використовується стовпчик *Cancer_Rate*, де всі значення є числовими, що дозволяє легко знайти середнє арифметичне за допомогою цієї функції. Метод `mean()` обчислює середнє значення, яке є однією з ключових статистичних величин для аналізу даних, оскільки показує центральну тенденцію значень у наборі даних.

Зверніть увагу, що отримане середнє значення має багато знаків після коми — 294.08200000000005. Це пояснюється тим, як працюють числа з плаваючою точкою (floating point numbers) в обчисленнях на комп'ютері. Числа з плаваючою точкою зберігаються в двійковому форматі, і через обмеження цього формату деякі числа не можуть бути представлені точно, що призводить до незначних помилок в обчисленнях. Ці відхилення стають помітними при відображенні результатів із великою кількістю десяткових знаків.

Числа з плаваючою точкою представлені в пам'яті як набір бітів, де частина бітів відповідає за мантису, частина — за експоненту, а частина — за знак. Це дозволяє представити як дуже великі, так і дуже малі числа, але при цьому точність обмежена розрядністю бітів, які використовуються. У стандарті IEEE 754 використовується 32 біти для звичайної точності і 64 біти для подвійної точності. Проте не всі десяткові дроби можуть бути точно представлені у двійковій системі, що спричиняє маленькі помилки, які акумулюються під час обчислень.

Посилання на теоретичну частину можна знайти в розділі [[Series.mean()]](#seriesmean).

### Виконаємо умовне завдання

Щоб продемонструвати декілька функцій одразу, виконаємо умовне завдання, де у нас є перемішані дані та відсутня колонка `Rank`. Нам потрібно відновити порядок та відсортувати дані за рівнем захворюваності на рак.

#### Видалення рангу та перемішування

Для цього ми використали функцію `DataFrame.drop()` для видалення колонки `Rank` та функцію `DataFrame.sample()` для перемішування даних.

Функція `drop()` дозволяє видалити колонку або рядок. В нашому випадку, ми видалили колонку за допомогою аргументу `columns=['Rank']`, що вказує на видалення конкретної колонки. Після цього застосовано `sample(frac=1)`, що перемішує рядки випадковим чином, де `frac=1` означає взяти всі рядки. Функція `reset_index(drop=True)` скидає індексацію, що дозволяє відновити правильний порядок рядків.

Цей процес продемонстровано на Рисунку 3.5.

![Рисунок 3.5 - Видалення рангу та перемішування](image-4.png)

Посилання на теоретичний матеріал щодо використання функцій:

- Теорія щодо `DataFrame.drop()` детально описана у відповідному розділі: [[DataFrame.drop()]](#dataframedrop)
- Інформація щодо `DataFrame.sample()` доступна тут: [[DataFrame.sample()]](#dataframesample)

#### Сортування та додавання рангу

Наступним кроком стало сортування даних за рівнем захворюваності за допомогою функції `DataFrame.sort_values()`. Аргумент `by='Cancer_Rate'` вказує колонку, за якою здійснюється сортування, а `ascending=False` вказує на сортування у порядку спадання. Після сортування ми використали функцію `range()`, щоб додати нову колонку `Rank` із послідовними значеннями для ранжування країн за рівнем захворюваності.

Весь процес продемонстровано на Рисунку 3.6.

![Рисунок 3.6 - Сортування та додавання рангу](image-5.png)

Посилання на теоретичний матеріал:

- Теорія щодо сортування даних описана у розділі: [DataFrame.sort_values()](#dataframesort_values)

### Додамо нову колонку

![Рисунок 3.7](image-6.png)

На Рисунку 3.7 показано процес створення нової колонки, де ми використовуємо функцію `apply()` для того, щоб пройтися по значенням у стовпці `Cancer_Rate` та застосувати функцію категоризації. Наша мета полягає у класифікації кожного рівня захворюваності на рак в одну з трьох категорій: високий, середній та низький рівні. Це допоможе зробити наші дані більш зрозумілими для подальшого аналізу.

Ми створили функцію `categorize_cancer_rate()`, яка використовує прості умови для перевірки рівня захворюваності та присвоєння відповідної категорії:

- Якщо рівень захворюваності більше ніж 350 — це категорія "High".
- Якщо більше ніж 300, але менше або дорівнює 350 — категорія "Medium".
- Усі інші значення — "Low".

Функція `apply()` дозволяє виконувати цю операцію для кожного значення у стовпці, перетворюючи їх на відповідні категорії. У результаті ми отримали новий стовпець, `Cancer_Rate_Category`, який містить ці категорії, як показано на Рисунку 3.7.

Ми використали функцію `apply()`, оскільки вона дозволяє застосувати довільну функцію до кожного значення в стовпці. Це ефективний спосіб для застосування складних логік на рівні кожного елемента, коли стандартні методи бібліотеки `pandas` можуть бути недостатніми. Як зазначалося в теоретичному матеріалі, функція `apply()` забезпечує гнучкість і простоту при роботі з великими даними та є важливим інструментом для створення нових колонок на основі складних правил.

![Рисунок 3.8 - Створена нова колонка](image-7.png)

Результат нашої операції дозволяє класифікувати країни за рівнем захворюваності на рак і значно полегшує подальший аналіз даних.

Для виведення першого, середнього та останнього рядка з нашого датасету, ми використали метод індексації через `iloc[]`. Цей метод дозволяє доступ до рядків і стовпців датасету за їхнім позиційним індексом. Як було показано на попередньому рисунку, ми застосували індекси:

- `[0]` для отримання першого рядка;
- `[10]` для отримання середнього рядка;
- `[-1]` для отримання останнього рядка (негативний індекс використовується для доступу до елементів з кінця масиву).

Метод індексації `iloc[]` є надзвичайно ефективним, оскільки працює з позиціями, а не з мітками. Цей спосіб доступу до даних вважається більш продуктивним при роботі з великими масивами даних, оскільки дозволяє швидко отримати доступ до конкретних рядків або стовпців на основі їхнього розташування в пам'яті.

Посилання на теоретичний матеріал щодо використання методу `iloc[]` та індексації у Python можна знайти в розділі теорії [[DataFrame.iloc]](#dataframeiloc), де описано роботу з індексацією масивів та DataFrame у бібліотеці `pandas`.

### Порахуємо скільки країн кожного типу

![Рисунок 3.9 - Використання value_counts](image-8.png)

На Рисунку 3.9 показано, як за допомогою методу `value_counts()` ми підрахували кількість країн з різними категоріями рівня захворюваності на рак. Функція `value_counts()` дозволяє швидко та ефективно отримати розподіл даних для конкретної категорії. В нашому випадку вона використовується для підрахунку, скільки країн мають низький, середній або високий рівень захворюваності.

Ми створили нову колонку `Cancer_Rate_Category`, яка містить категорії 'Low', 'Medium', 'High', як було показано раніше. Після цього, застосувавши `value_counts()`, ми отримали розподіл цих категорій:

- 30 країн мають низький рівень захворюваності на рак.
- 15 країн мають середній рівень.
- 5 країн мають високий рівень.

Метод `value_counts()` оптимізований для роботи з великими наборами даних і має лінійну складність, оскільки він проходить по кожному елементу в серії і підраховує кількість унікальних значень.

Посилання на теоретичні відомості щодо методу `value_counts()` ви знайдете у відповідному розділі [[Series.value_counts()]](#seriesvalue_counts).

## Висновок

У процесі виконання лабораторної роботи було досягнуто поставлену мету – навчитися вибирати відповідні дані з DataFrame або Series, а також вибирати правильні дані з набору даних.

На першому етапі ми підготували теоретичні відомості, які допомогли краще зрозуміти основні концепції роботи з мовою Python, а також важливі аспекти бібліотек NumPy і Pandas. Це стало основою для подальших практичних завдань, зокрема, завантаження даних та їх обробки.

Далі, для виконання лабораторної роботи, ми встановили Anaconda, що значно полегшило роботу з інструментами Python, включаючи Jupyter Notebook для інтерактивної обробки даних.

У ході роботи ми застосували знання з програмування та операційних систем, зокрема, роботу з пам'яттю та обчислювальну ефективність на рівні C, оскільки бібліотеки Pandas і NumPy використовують низькорівневі оптимізації для ефективної обробки великих обсягів даних.

Практичні завдання, які ми виконали, включали завантаження набору даних за допомогою методу `read_csv()`, його попередній перегляд через `head()`, а також опис параметрів даних за допомогою `describe()`. Окрім цього, ми здійснили сортування та вибір даних за допомогою `sort_values()` та додали нові категорії через метод `apply()`.

Одним із важливих моментів було використання індексації для доступу до конкретних рядків у датафреймі, зокрема, до першого, середнього та останнього рядків. Ми також підрахували унікальні значення в колонках, що дало змогу визначити кількість країн у кожній категорії.

Підсумовуючи, всі етапи роботи були успішно виконані, що дало змогу не лише на практиці застосувати отримані знання, а й отримати глибше розуміння роботи з даними у Python. Мета лабораторної роботи – навчитися вибирати відповідні дані з DataFrame або Series та правильно працювати з набором даних – була повністю досягнута.
