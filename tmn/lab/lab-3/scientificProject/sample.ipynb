{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Лабораторна робота №3\n",
    "\n",
    "## Тема\n",
    "\n",
    "Вступ до опрацювання природної мови\n",
    "\n",
    "## Мета\n",
    "\n",
    "здобуття студентом навичок реалізації базових методів обробки\n",
    "природної мови, включно з попереднім опрацюванням тексту, формуванням «мішка\n",
    "слів» («bag-of-words»), виділенням стоп-слів і найважливіших слів у документі,\n",
    "створенням тематичних моделей.\n",
    "\n",
    "## Автор\n",
    "\n",
    "Молчанов Михайло, ІА-12\n",
    "\n",
    "## Виконання"
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Завантаження та зчитування тексту\n",
    "\n",
    "У цьому блоці коду ми реалізуємо завантаження англійської книги *\"Alice's Adventures in Wonderland\"* із проекту Gutenberg. Основні дії включають:\n",
    "\n",
    "- **Завантаження тексту** за URL за допомогою бібліотеки `requests`.\n",
    "- **Збереження тексту** у локальному файлі `alice_in_wonderland.txt` з використанням кодування `utf-8`.\n",
    "- **Зчитування тексту** з файлу у змінну `text` для подальшого опрацювання.\n"
   ],
   "id": "9b9a3d262c7acab2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T20:50:51.447876Z",
     "start_time": "2024-11-22T20:50:50.325964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Завантаження книги\n",
    "import requests\n",
    "\n",
    "url = \"http://www.gutenberg.org/files/11/11-0.txt\"\n",
    "response = requests.get(url)\n",
    "with open(\"data/alice_in_wonderland.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(response.text)\n",
    "\n",
    "# Зчитування тексту\n",
    "with open(\"data/alice_in_wonderland.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Результат виконання:\n",
    "\n",
    "- Текст книги успішно завантажено з інтернету та збережено в локальному файлі.\n",
    "- Тепер повний текст доступний у змінній `text`, і ми можемо почати його обробку.\n"
   ],
   "id": "38412c6cd07d6ffe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Попередня обробка тексту\n",
    "\n",
    "На цьому етапі виконується попереднє опрацювання тексту, яке включає:\n",
    "\n",
    "1. **Імпорт бібліотек:**\n",
    "   - `nltk` для токенізації та роботи зі стоп-словами.\n",
    "   - `re` для роботи з регулярними виразами.\n",
    "2. **Завантаження необхідних даних:**\n",
    "   - Завантажуються стоп-слова та токенізатор із бібліотеки `nltk`.\n",
    "3. **Реалізація функції `preprocess_text`:**\n",
    "   - Приведення тексту до нижнього регістру.\n",
    "   - Видалення цифр за допомогою регулярного виразу.\n",
    "   - Видалення пунктуації та неалфавітних символів.\n",
    "   - Токенізація тексту, тобто розбиття на окремі слова.\n",
    "   - Видалення стоп-слів і неалфавітних токенів.\n"
   ],
   "id": "4b066c681d4d6313"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T20:50:51.515907Z",
     "start_time": "2024-11-22T20:50:51.460054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Попередня обробка тексту\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # до нижнього регістру\n",
    "    text = re.sub(r'\\d+', '', text)  # видалення цифр\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # видалення пунктуації\n",
    "    tokens = word_tokenize(text)  # токенізація\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words and word.isalpha()]  # видалення стоп-слів\n",
    "    return filtered_tokens\n",
    "\n",
    "processed_text = preprocess_text(text)\n"
   ],
   "id": "2c9b633a781eccf3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/killer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/killer/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Результат виконання:\n",
    "\n",
    "- Текст очищено від зайвих символів, цифр і стоп-слів. Він представлений у вигляді списку токенів, які готові до подальшого аналізу.\n",
    "- `processed_text` містить попередньо оброблений текст книги, що є зручним форматом для роботи з алгоритмами NLP.\n"
   ],
   "id": "59ab3e81fc4d2b74"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Розділення тексту на глави\n",
    "\n",
    "У цьому блоці коду текст книги розділяється на окремі глави для подальшого аналізу. Основні кроки:\n",
    "\n",
    "1. **Розділення тексту:**\n",
    "   - Використовується метод `split(\"CHAPTER\")` для поділу тексту на частини на основі ключового слова \"CHAPTER\".\n",
    "   - Пропускається перший елемент (`[1:]`), щоб уникнути тексту перед першою главою.\n",
    "2. **Відновлення заголовків:**\n",
    "   - До кожної частини тексту додається слово \"CHAPTER\", щоб зберегти формат глав.\n",
    "3. **Попередня обробка:**\n",
    "   - Кожна глава обробляється за допомогою функції `preprocess_text`, описаної раніше, що очищує текст від зайвих символів, цифр та стоп-слів.\n"
   ],
   "id": "12df4397abd652a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T20:50:51.563666Z",
     "start_time": "2024-11-22T20:50:51.523271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Розділення на глави\n",
    "chapters = text.split(\"CHAPTER\")[1:]  # Розділення тексту по главам\n",
    "chapters = [\"CHAPTER\" + chapter for chapter in chapters]  # Додавання заголовка назад\n",
    "preprocessed_chapters = [preprocess_text(chapter) for chapter in chapters]\n"
   ],
   "id": "c7230c2bed1c9afe",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Результат виконання:\n",
    "\n",
    "- Текст книги успішно розділено на глави, а також кожна глава очищена та представлена у вигляді списку токенів.\n",
    "- Змінна `preprocessed_chapters` містить список оброблених глав, готових до аналізу за алгоритмами TF-IDF та LDA.\n"
   ],
   "id": "5e913f02be014c6b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Визначення топ-20 слів для кожної глави за TF-IDF\n",
    "\n",
    "У цьому блоці коду виконується обчислення топ-20 ключових слів для кожної глави книги на основі алгоритму TF-IDF (Term Frequency-Inverse Document Frequency). Основні кроки:\n",
    "\n",
    "1. **Імпорт необхідних бібліотек:**\n",
    "   - Використовується `TfidfVectorizer` з `scikit-learn`.\n",
    "2. **Ініціалізація векторизатора:**\n",
    "   - Параметр `max_features=20` обмежує кількість ключових слів до 20 для кожної глави.\n",
    "3. **Обчислення TF-IDF:**\n",
    "   - Текст кожної глави об'єднується в одну строку.\n",
    "   - `fit_transform` обчислює ваги TF-IDF для кожного слова.\n",
    "   - За допомогою `get_feature_names_out()` отримуються ключові слова, а з матриці TF-IDF витягуються їхні відповідні значення.\n",
    "4. **Сортування результатів:**\n",
    "   - Слова сортуються за вагами TF-IDF у порядку спадання.\n",
    "\n",
    "\n",
    "### Як працює TF-IDF\n",
    "\n",
    "1. **Term Frequency (TF):**\n",
    "   - Оцінює частоту появи слова у тексті глави. Чим частіше слово зустрічається, тим більший TF.\n",
    "   $\n",
    "   TF(t) = \\frac{\\text{Кількість появ слова } t \\text{ у главі}}{\\text{Загальна кількість слів у главі}}\n",
    "   $\n",
    "2. **Inverse Document Frequency (IDF):**\n",
    "   - Зменшує важливість часто вживаних слів (на кшталт \"the\", \"and\"), які зустрічаються у багатьох документах.\n",
    "   $\n",
    "   IDF(t) = \\log{\\frac{\\text{Загальна кількість глав}}{1 + \\text{Кількість глав, у яких зустрічається } t}}\n",
    "   $\n",
    "3. **Розрахунок TF-IDF:**\n",
    "   - Вага TF-IDF для слова обчислюється як добуток TF та IDF.\n",
    "   $\n",
    "   TF\\text{-}IDF(t) = TF(t) \\times IDF(t)\n",
    "   $\n"
   ],
   "id": "21da9f03917caf4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T20:50:51.603774Z",
     "start_time": "2024-11-22T20:50:51.573609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Визначення топ-20 слів для кожної глави за TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=20)\n",
    "tfidf_results = []\n",
    "\n",
    "for chapter in preprocessed_chapters:\n",
    "    chapter_text = \" \".join(chapter)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([chapter_text])\n",
    "    tfidf_terms = tfidf_vectorizer.get_feature_names_out()\n",
    "    tfidf_scores = tfidf_matrix.toarray()[0]\n",
    "    tfidf_results.append(sorted(zip(tfidf_terms, tfidf_scores), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "tfidf_results\n"
   ],
   "id": "b36aa89886b08ca1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('chapter', 0.7071067811865475), ('rabbithole', 0.7071067811865475)],\n",
       " [('chapter', 0.5), ('ii', 0.5), ('pool', 0.5), ('tears', 0.5)],\n",
       " [('caucusrace', 0.4472135954999579),\n",
       "  ('chapter', 0.4472135954999579),\n",
       "  ('iii', 0.4472135954999579),\n",
       "  ('long', 0.4472135954999579),\n",
       "  ('tale', 0.4472135954999579)],\n",
       " [('bill', 0.4082482904638631),\n",
       "  ('chapter', 0.4082482904638631),\n",
       "  ('iv', 0.4082482904638631),\n",
       "  ('little', 0.4082482904638631),\n",
       "  ('rabbit', 0.4082482904638631),\n",
       "  ('sends', 0.4082482904638631)],\n",
       " [('advice', 0.5773502691896258),\n",
       "  ('caterpillar', 0.5773502691896258),\n",
       "  ('chapter', 0.5773502691896258)],\n",
       " [('chapter', 0.5), ('pepper', 0.5), ('pig', 0.5), ('vi', 0.5)],\n",
       " [('chapter', 0.5), ('mad', 0.5), ('teaparty', 0.5), ('vii', 0.5)],\n",
       " [('chapter', 0.5), ('croquetground', 0.5), ('queens', 0.5), ('viii', 0.5)],\n",
       " [('chapter', 0.4472135954999579),\n",
       "  ('ix', 0.4472135954999579),\n",
       "  ('mock', 0.4472135954999579),\n",
       "  ('story', 0.4472135954999579),\n",
       "  ('turtles', 0.4472135954999579)],\n",
       " [('chapter', 0.5773502691896258),\n",
       "  ('lobster', 0.5773502691896258),\n",
       "  ('quadrille', 0.5773502691896258)],\n",
       " [('chapter', 0.5), ('stole', 0.5), ('tarts', 0.5), ('xi', 0.5)],\n",
       " [('alices', 0.5), ('chapter', 0.5), ('evidence', 0.5), ('xii', 0.5)],\n",
       " [('alice', 0.5820263283049563),\n",
       "  ('little', 0.32334796016942013),\n",
       "  ('like', 0.23712183745757479),\n",
       "  ('think', 0.23712183745757479),\n",
       "  ('way', 0.23712183745757479),\n",
       "  ('see', 0.21556530677961344),\n",
       "  ('could', 0.17245224542369075),\n",
       "  ('one', 0.17245224542369075),\n",
       "  ('said', 0.17245224542369075),\n",
       "  ('thought', 0.17245224542369075),\n",
       "  ('time', 0.17245224542369075),\n",
       "  ('door', 0.1508957147457294),\n",
       "  ('eat', 0.1508957147457294),\n",
       "  ('found', 0.1508957147457294),\n",
       "  ('get', 0.1508957147457294),\n",
       "  ('nothing', 0.1508957147457294),\n",
       "  ('well', 0.1508957147457294),\n",
       "  ('went', 0.1508957147457294),\n",
       "  ('key', 0.12933918406776806),\n",
       "  ('rabbit', 0.12933918406776806)],\n",
       " [('alice', 0.4865642399068221),\n",
       "  ('little', 0.3243761599378814),\n",
       "  ('mouse', 0.3243761599378814),\n",
       "  ('im', 0.24328211995341106),\n",
       "  ('said', 0.24328211995341106),\n",
       "  ('dear', 0.2027350999611759),\n",
       "  ('go', 0.2027350999611759),\n",
       "  ('like', 0.1824615899650583),\n",
       "  ('must', 0.1824615899650583),\n",
       "  ('oh', 0.1824615899650583),\n",
       "  ('thought', 0.1824615899650583),\n",
       "  ('went', 0.1824615899650583),\n",
       "  ('cried', 0.1621880799689407),\n",
       "  ('feet', 0.1621880799689407),\n",
       "  ('pool', 0.1621880799689407),\n",
       "  ('way', 0.1621880799689407),\n",
       "  ('cats', 0.14191456997282312),\n",
       "  ('could', 0.14191456997282312),\n",
       "  ('ill', 0.14191456997282312),\n",
       "  ('one', 0.14191456997282312)],\n",
       " [('said', 0.6425396041156863),\n",
       "  ('alice', 0.43465914396061134),\n",
       "  ('mouse', 0.35906624935876585),\n",
       "  ('dodo', 0.22677868380553634),\n",
       "  ('know', 0.20788046015507497),\n",
       "  ('one', 0.13228756555322954),\n",
       "  ('soon', 0.13228756555322954),\n",
       "  ('dry', 0.11338934190276817),\n",
       "  ('long', 0.11338934190276817),\n",
       "  ('lory', 0.11338934190276817),\n",
       "  ('round', 0.11338934190276817),\n",
       "  ('thing', 0.11338934190276817),\n",
       "  ('would', 0.11338934190276817),\n",
       "  ('birds', 0.09449111825230681),\n",
       "  ('could', 0.09449111825230681),\n",
       "  ('little', 0.09449111825230681),\n",
       "  ('must', 0.09449111825230681),\n",
       "  ('prizes', 0.09449111825230681),\n",
       "  ('question', 0.09449111825230681),\n",
       "  ('quite', 0.09449111825230681)],\n",
       " [('alice', 0.5559369874958259),\n",
       "  ('little', 0.42621835708013317),\n",
       "  ('one', 0.2594372608313854),\n",
       "  ('said', 0.2594372608313854),\n",
       "  ('rabbit', 0.20384356208180282),\n",
       "  ('bill', 0.16678109624874776),\n",
       "  ('get', 0.16678109624874776),\n",
       "  ('heard', 0.16678109624874776),\n",
       "  ('sure', 0.16678109624874776),\n",
       "  ('thought', 0.16678109624874776),\n",
       "  ('came', 0.14824986333222023),\n",
       "  ('made', 0.14824986333222023),\n",
       "  ('quite', 0.14824986333222023),\n",
       "  ('thing', 0.14824986333222023),\n",
       "  ('window', 0.14824986333222023),\n",
       "  ('great', 0.1297186304156927),\n",
       "  ('like', 0.1297186304156927),\n",
       "  ('moment', 0.1297186304156927),\n",
       "  ('room', 0.1297186304156927),\n",
       "  ('went', 0.1297186304156927)],\n",
       " [('said', 0.6893617944559562),\n",
       "  ('alice', 0.4639935154992013),\n",
       "  ('caterpillar', 0.33142393964228667),\n",
       "  ('im', 0.198854363785372),\n",
       "  ('pigeon', 0.1590834910282976),\n",
       "  ('little', 0.13256957585691467),\n",
       "  ('well', 0.13256957585691467),\n",
       "  ('serpent', 0.1193126182712232),\n",
       "  ('ive', 0.10605566068553174),\n",
       "  ('know', 0.09279870309984027),\n",
       "  ('think', 0.09279870309984027),\n",
       "  ('bit', 0.0795417455141488),\n",
       "  ('cant', 0.0795417455141488),\n",
       "  ('got', 0.0795417455141488),\n",
       "  ('last', 0.0795417455141488),\n",
       "  ('see', 0.0795417455141488),\n",
       "  ('size', 0.0795417455141488),\n",
       "  ('tried', 0.0795417455141488),\n",
       "  ('youre', 0.0795417455141488),\n",
       "  ('youth', 0.0795417455141488)],\n",
       " [('said', 0.581533542869141),\n",
       "  ('alice', 0.5320413264547461),\n",
       "  ('cat', 0.2722071902791724),\n",
       "  ('like', 0.19796886565757993),\n",
       "  ('duchess', 0.17322275745038243),\n",
       "  ('little', 0.17322275745038243),\n",
       "  ('much', 0.14847664924318496),\n",
       "  ('baby', 0.1361035951395862),\n",
       "  ('mad', 0.1361035951395862),\n",
       "  ('would', 0.1361035951395862),\n",
       "  ('footman', 0.12373054103598746),\n",
       "  ('know', 0.12373054103598746),\n",
       "  ('thought', 0.12373054103598746),\n",
       "  ('went', 0.12373054103598746),\n",
       "  ('could', 0.11135748693238871),\n",
       "  ('get', 0.11135748693238871),\n",
       "  ('see', 0.11135748693238871),\n",
       "  ('large', 0.09898443282878996),\n",
       "  ('quite', 0.09898443282878996),\n",
       "  ('way', 0.09898443282878996)],\n",
       " [('said', 0.5863074165272358),\n",
       "  ('alice', 0.5054374280407206),\n",
       "  ('hatter', 0.3234799539460611),\n",
       "  ('dormouse', 0.2628274625811747),\n",
       "  ('hare', 0.21228371977710261),\n",
       "  ('march', 0.21228371977710261),\n",
       "  ('time', 0.17184872553384498),\n",
       "  ('know', 0.14152247985140173),\n",
       "  ('well', 0.12130498272977293),\n",
       "  ('one', 0.1010874856081441),\n",
       "  ('went', 0.1010874856081441),\n",
       "  ('little', 0.09097873704732969),\n",
       "  ('say', 0.09097873704732969),\n",
       "  ('thing', 0.09097873704732969),\n",
       "  ('dont', 0.07076123992570087),\n",
       "  ('replied', 0.07076123992570087),\n",
       "  ('tea', 0.07076123992570087),\n",
       "  ('think', 0.07076123992570087),\n",
       "  ('like', 0.060652491364886464),\n",
       "  ('twinkle', 0.060652491364886464)],\n",
       " [('said', 0.5658874543713567),\n",
       "  ('alice', 0.513246760941463),\n",
       "  ('queen', 0.40796537408167577),\n",
       "  ('head', 0.15792208028968094),\n",
       "  ('king', 0.14476190693220753),\n",
       "  ('three', 0.14476190693220753),\n",
       "  ('one', 0.13160173357473412),\n",
       "  ('went', 0.13160173357473412),\n",
       "  ('began', 0.11844156021726071),\n",
       "  ('came', 0.11844156021726071),\n",
       "  ('cat', 0.11844156021726071),\n",
       "  ('like', 0.11844156021726071),\n",
       "  ('see', 0.11844156021726071),\n",
       "  ('soldiers', 0.11844156021726071),\n",
       "  ('two', 0.11844156021726071),\n",
       "  ('dont', 0.10528138685978729),\n",
       "  ('gardeners', 0.10528138685978729),\n",
       "  ('looked', 0.10528138685978729),\n",
       "  ('could', 0.09212121350231388),\n",
       "  ('hedgehog', 0.09212121350231388)],\n",
       " [('said', 0.6102629825251079),\n",
       "  ('alice', 0.5031993013803522),\n",
       "  ('mock', 0.278365570976365),\n",
       "  ('turtle', 0.278365570976365),\n",
       "  ('gryphon', 0.21412736228951154),\n",
       "  ('duchess', 0.1927146260605604),\n",
       "  ('queen', 0.1391827854881825),\n",
       "  ('went', 0.1391827854881825),\n",
       "  ('dont', 0.10706368114475577),\n",
       "  ('little', 0.10706368114475577),\n",
       "  ('never', 0.10706368114475577),\n",
       "  ('know', 0.0963573130302802),\n",
       "  ('say', 0.0963573130302802),\n",
       "  ('day', 0.08565094491580462),\n",
       "  ('like', 0.08565094491580462),\n",
       "  ('moral', 0.08565094491580462),\n",
       "  ('quite', 0.08565094491580462),\n",
       "  ('come', 0.07494457680132904),\n",
       "  ('thought', 0.07494457680132904),\n",
       "  ('much', 0.06423820868685347)],\n",
       " [('said', 0.5468321485616328),\n",
       "  ('gryphon', 0.37670659123134703),\n",
       "  ('alice', 0.35240294018416335),\n",
       "  ('mock', 0.3402511146605715),\n",
       "  ('turtle', 0.3402511146605715),\n",
       "  ('would', 0.1822773828538776),\n",
       "  ('dance', 0.1579737318066939),\n",
       "  ('beautiful', 0.14582190628310207),\n",
       "  ('soup', 0.13367008075951023),\n",
       "  ('voice', 0.12151825523591839),\n",
       "  ('wont', 0.12151825523591839),\n",
       "  ('join', 0.10936642971232655),\n",
       "  ('know', 0.09721460418873472),\n",
       "  ('like', 0.09721460418873472),\n",
       "  ('whiting', 0.09721460418873472),\n",
       "  ('come', 0.08506277866514288),\n",
       "  ('could', 0.08506277866514288),\n",
       "  ('first', 0.08506277866514288),\n",
       "  ('lobster', 0.08506277866514288),\n",
       "  ('sea', 0.08506277866514288)],\n",
       " [('said', 0.6237828615518053),\n",
       "  ('king', 0.4054588600086734),\n",
       "  ('hatter', 0.32748600231469777),\n",
       "  ('alice', 0.24951314462072213),\n",
       "  ('court', 0.23391857308192698),\n",
       "  ('dormouse', 0.2027294300043367),\n",
       "  ('one', 0.17154028692674644),\n",
       "  ('witness', 0.15594571538795132),\n",
       "  ('queen', 0.1403511438491562),\n",
       "  ('rabbit', 0.12475657231036107),\n",
       "  ('began', 0.10916200077156593),\n",
       "  ('im', 0.10916200077156593),\n",
       "  ('thought', 0.10916200077156593),\n",
       "  ('white', 0.10916200077156593),\n",
       "  ('march', 0.0935674292327708),\n",
       "  ('next', 0.0935674292327708),\n",
       "  ('voice', 0.0935674292327708),\n",
       "  ('breadandbutter', 0.07797285769397566),\n",
       "  ('little', 0.07797285769397566),\n",
       "  ('see', 0.07797285769397566)],\n",
       " [('said', 0.7428018440762046),\n",
       "  ('alice', 0.32683281139353004),\n",
       "  ('king', 0.32683281139353004),\n",
       "  ('would', 0.17827244257828911),\n",
       "  ('jury', 0.14856036881524093),\n",
       "  ('little', 0.14856036881524093),\n",
       "  ('know', 0.13370433193371684),\n",
       "  ('queen', 0.13370433193371684),\n",
       "  ('one', 0.11884829505219274),\n",
       "  ('rabbit', 0.11884829505219274),\n",
       "  ('white', 0.11884829505219274),\n",
       "  ('could', 0.10399225817066865),\n",
       "  ('gave', 0.10399225817066865),\n",
       "  ('head', 0.10399225817066865),\n",
       "  ('nothing', 0.10399225817066865),\n",
       "  ('dream', 0.08913622128914456),\n",
       "  ('sister', 0.08913622128914456),\n",
       "  ('theres', 0.08913622128914456),\n",
       "  ('voice', 0.08913622128914456),\n",
       "  ('thats', 0.07428018440762046)]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Результат виконання\n",
    "\n",
    "- Для кожної глави визначено топ-20 ключових слів із відповідними вагами TF-IDF.\n",
    "- Наприклад, у першій главі слова \"chapter\" і \"rabbithole\" мають найбільшу вагу, що вказує на їхню значущість у контексті цієї глави.\n",
    "\n",
    "TF-IDF дозволяє ідентифікувати унікальні слова для кожного розділу, відфільтровуючи загальні терміни. Це ефективний метод для аналізу тексту та виявлення ключових слів.\n"
   ],
   "id": "fbb288de4006b240"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Реалізація LDA (Latent Dirichlet Allocation)\n",
    "У цьому блоці коду реалізується тематичне моделювання тексту книги за допомогою алгоритму **LDA** (Latent Dirichlet Allocation). Основні кроки:\n",
    "\n",
    "1. **Імпорт бібліотек:**\n",
    "   - `LatentDirichletAllocation` для реалізації алгоритму LDA.\n",
    "   - `CountVectorizer` для перетворення тексту в числову матрицю частот слів.\n",
    "\n",
    "2. **Векторизація тексту:**\n",
    "   - Використовується `CountVectorizer` для побудови матриці частот слів.\n",
    "   - Параметр `max_features=1000` обмежує кількість унікальних слів, які враховуються, до 1000.\n",
    "   - Текст усіх глав об'єднується в один список, де кожна глава — це окремий документ.\n",
    "\n",
    "3. **Навчання LDA:**\n",
    "   - Алгоритм LDA налаштовується на знаходження 5 тем (`n_components=5`) у тексті.\n",
    "   - `random_state=42` забезпечує відтворюваність результатів.\n",
    "   - `lda.fit` навчає модель на матриці частот слів.\n",
    "\n",
    "4. **Отримання топ-слів для кожної теми:**\n",
    "   - `lda.components_` повертає ваги слів для кожної теми.\n",
    "   - Найважливіші слова для кожної теми сортуються за їхніми вагами та відбирається топ-20 слів.\n",
    "\n",
    "### Як працює LDA:\n",
    "1. **Ідея алгоритму:**\n",
    "   - LDA — це алгоритм, що припускає, що кожен документ (глава) є сумішшю різних тем, а кожна тема — це набір слів із певними ймовірностями.\n",
    "   - Наприклад, у тексті книги можуть бути теми, пов'язані з персонажами, місцями або подіями.\n",
    "\n",
    "2. **Розподіл тем:**\n",
    "   - LDA визначає, які теми найкраще описують кожну главу, і які слова характерні для цих тем.\n",
    "\n",
    "3. **Побудова тем:**\n",
    "   - Кожне слово в тексті отримує вагу в кожній темі. Топ-слова з найвищими вагами визначають характер теми.\n"
   ],
   "id": "2988fa7c4a3b0147"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T20:50:51.670434Z",
     "start_time": "2024-11-22T20:50:51.614798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Реалізація LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
    "lda_matrix = vectorizer.fit_transform([\" \".join(chapter) for chapter in preprocessed_chapters])\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(lda_matrix)\n",
    "\n",
    "lda_topics = lda.components_\n",
    "lda_terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Отримання топ-слів для кожної теми\n",
    "lda_results = []\n",
    "for topic_idx, topic in enumerate(lda_topics):\n",
    "    lda_results.append(sorted(zip(lda_terms, topic), key=lambda x: x[1], reverse=True)[:20])\n",
    "\n",
    "lda_results\n"
   ],
   "id": "cb1efe8d0e94e44e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('alice', 44.2826729486036),\n",
       "  ('little', 41.81616526203385),\n",
       "  ('rabbit', 17.610056170840295),\n",
       "  ('thought', 17.173069278184247),\n",
       "  ('said', 16.032227421457005),\n",
       "  ('like', 15.115756211127685),\n",
       "  ('oh', 14.191846970110726),\n",
       "  ('sure', 14.074789348830507),\n",
       "  ('door', 13.268383941349947),\n",
       "  ('moment', 13.014584779230438),\n",
       "  ('way', 12.796301840215294),\n",
       "  ('quite', 12.69138537382185),\n",
       "  ('went', 12.56874231989401),\n",
       "  ('came', 12.487646314394004),\n",
       "  ('wonder', 11.501106785710798),\n",
       "  ('heard', 11.281867911916308),\n",
       "  ('im', 10.689982437206343),\n",
       "  ('thing', 10.506302943519906),\n",
       "  ('feet', 10.197633292998859),\n",
       "  ('fan', 10.19707560495302)],\n",
       " [('said', 115.62880593656809),\n",
       "  ('alice', 97.43771248039693),\n",
       "  ('hatter', 53.19293736342241),\n",
       "  ('dormouse', 39.19599694001947),\n",
       "  ('mouse', 35.020109164234846),\n",
       "  ('know', 32.20895191123559),\n",
       "  ('march', 28.181248113307365),\n",
       "  ('hare', 27.181326282971057),\n",
       "  ('time', 24.457723817277486),\n",
       "  ('little', 22.534506741095853),\n",
       "  ('went', 19.932960827782317),\n",
       "  ('im', 19.393398936958814),\n",
       "  ('thing', 18.129437828562132),\n",
       "  ('say', 16.70620979241015),\n",
       "  ('thought', 15.75376169412255),\n",
       "  ('ill', 15.279899901208982),\n",
       "  ('like', 14.768037989680304),\n",
       "  ('long', 14.059191391307365),\n",
       "  ('quite', 13.769518923638131),\n",
       "  ('think', 13.495242647334376)],\n",
       " [('chapter', 0.20019960745764015),\n",
       "  ('advice', 0.20009664690711979),\n",
       "  ('story', 0.20009647841992761),\n",
       "  ('ix', 0.20009578357822594),\n",
       "  ('viii', 0.20006992077644922),\n",
       "  ('vi', 0.20006987849055935),\n",
       "  ('rabbithole', 0.2000527460467316),\n",
       "  ('alices', 0.2000515892855501),\n",
       "  ('vii', 0.20004637213879356),\n",
       "  ('stole', 0.20004567043081395),\n",
       "  ('croquetground', 0.20003272885247544),\n",
       "  ('ii', 0.2000306132261149),\n",
       "  ('mad', 0.2000305763851621),\n",
       "  ('teaparty', 0.20002955143487766),\n",
       "  ('quadrille', 0.20002635953446443),\n",
       "  ('iv', 0.20002317211381374),\n",
       "  ('sends', 0.20002317211381374),\n",
       "  ('caucusrace', 0.20002179533762024),\n",
       "  ('tale', 0.2000217422569831),\n",
       "  ('turtles', 0.2000202378404216)],\n",
       " [('said', 129.93878139342326),\n",
       "  ('alice', 79.87970526465892),\n",
       "  ('king', 48.19896672294101),\n",
       "  ('gryphon', 35.20007187135801),\n",
       "  ('mock', 31.199348747686038),\n",
       "  ('turtle', 29.200122511183956),\n",
       "  ('little', 25.249471395250186),\n",
       "  ('voice', 21.729121455649587),\n",
       "  ('know', 21.36890627735818),\n",
       "  ('rabbit', 20.78793113385636),\n",
       "  ('like', 17.716184115834142),\n",
       "  ('queen', 17.648554612659666),\n",
       "  ('white', 17.5741450299224),\n",
       "  ('court', 17.197678367500615),\n",
       "  ('went', 16.0983242076125),\n",
       "  ('time', 15.29690569029422),\n",
       "  ('began', 15.17076618443896),\n",
       "  ('jury', 14.858817153719308),\n",
       "  ('way', 14.500359828907172),\n",
       "  ('thats', 13.67716338037016)],\n",
       " [('said', 199.2001835694163),\n",
       "  ('alice', 164.19990763298185),\n",
       "  ('queen', 48.20038534573198),\n",
       "  ('like', 38.20002000364099),\n",
       "  ('little', 38.19985392236048),\n",
       "  ('duchess', 35.200188242902605),\n",
       "  ('went', 35.19997097475394),\n",
       "  ('cat', 31.200596408078923),\n",
       "  ('dont', 30.200115390642388),\n",
       "  ('thought', 29.19988923708741),\n",
       "  ('know', 28.199820441360433),\n",
       "  ('mock', 26.20064179044216),\n",
       "  ('caterpillar', 26.200040541541565),\n",
       "  ('turtle', 26.199873390560384),\n",
       "  ('im', 24.199947195453618),\n",
       "  ('head', 21.200012076741213),\n",
       "  ('quite', 21.19992825248987),\n",
       "  ('tone', 20.200080690700894),\n",
       "  ('say', 20.19998318280051),\n",
       "  ('began', 20.199879889868274)]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Результат виконання:\n",
    "- Для кожної з 5 тем алгоритм визначив топ-20 слів.\n",
    "- Наприклад, у першій темі слова \"alice\", \"little\", \"rabbit\" мають найбільші ваги, що вказує на те, що ця тема пов'язана з головним персонажем і її взаємодією з кроликом.\n",
    "- Тематичне моделювання дає змогу зрозуміти основні ідеї та контекст тексту."
   ],
   "id": "bbb7e0e99bdf4f07"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Висновок\n",
    "\n",
    "На цій лабораторній роботі я здобув навички реалізації базових методів обробки\n",
    "природної мови, включно з попереднім опрацюванням тексту, формуванням «мішка\n",
    "слів» («bag-of-words»), виділенням стоп-слів і найважливіших слів у документі,\n",
    "створив тематичні моделі."
   ],
   "id": "b8119e14e6fd3a32"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
